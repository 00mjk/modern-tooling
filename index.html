<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Modern Tooling</title>
    <script src="https://www.w3.org/Tools/respec/respec-w3c-common" async class="remove"></script>
    <script class="remove">
      var respecConfig = {
              specStatus:   "webspec"
          ,   shortName:    "modern-tooling"
          ,   editors:      [{ name: "Robin Berjon", url: "http://berjon.com/",
                               company: "W3C", companyURL: "http://w3.org/" }]
          ,   edDraftURI:   "http://w3c.github.io/modern-tooling/"
          ,   license:      "w3c-software"
          ,   repository:   "w3c/modern-tooling"
        };
    </script>
  </head>
  <body>
    <section id="abstract">
      <p>
        This document is meant to capture the state of the discussion about modernising the tooling
        that supports the making of W3C standards.
      </p>
      <p>
        Discussion is open to all. <a href="https://github.com/w3c/modern-tooling/">Pull requests</a>
        are welcome, and the discussion takes place in
        <a href="https://github.com/w3c/modern-tooling/issues">the issues</a>.
      </p>
    </section>
    <section id="sotd">
      <p>
        This is just informal work by an informal task force.
      </p>
    </section>

    <section>
      <h2>Problem Statement</h2>
      <p>
        W3C is a heavily tooled organisation. We did Web tools long before they became ubiquitous
        and our tooling has earned us two decades of praise — which is probably more than can be
        said for any other part of the organisation.
      </p>
      <p>
        But the Web has changed quite a fair bit since then. Today, tools abound. The Web is an
        application platform. Our users are happy using many third-party tools, and it is only
        natural that they will compare what they can find elsewhere to our offering, sometimes
        preferring other options.
      </p>
      <p>
        This causes problems:
      </p>
      <ul>
        <li>
          Our users move their work to third-party tools, where the data they produce in developing
          specifications may be lost or locked up. GitHub is a primary example here.
        </li>
        <li>
          The work that groups carry out is becoming increasingly scattered across services, such
          that it is hard to track and hard to join. Issue tracking for instance is all over the
          place.
        </li>
        <li>
          Our tools, even when they work well and remain the preferred option, are starting to look
          old in both styling and interaction style. It is hard for our users to maintain the
          impression that they are building the Web of tomorrow while using tools that are clearly
          from the 90s. Some of our more recent users even find them hard to understand. A good
          example is <acronym title="Web-Based Straw-poll and balloting system">WBS</acronym> or 
          the mailing list archives.
        </li>
        <li>
          Accumulation is hard to manage and makes content hard to find. The W3C Web site and the
          majority of its services are exposed essentially as one single entity. After twenty years
          of accretion, that makes it hard to evolve as was made obvious in the recent discussion
          about upgrading it all to HTTPS. The heavily granular usage of ACLs makes it essentially
          impossible to index. Finding things on the W3C Web site typically involves knowing someone
          with the URL equivalent of an eidetic memory and asking them where to go. This issue is
          further compounded by user-hostile URL policies (datedspace). A good example here would
          be trying to find the next-generation pubrules service.
        </li>
      </ul>
      <p>
        We need an energetic rethink of the W3C’s tooling in order to stay relevant to our users.
        That is what this document endeavours to provide.
      </p>
      <section>
        <h2>The Plan</h2>
        <p>
          I expect to proceed as follows:
        </p>
        <ul>
          <li>
            Iterate over this document through
            <a href="https://github.com/w3c/modern-tooling/issues">issues</a> and
            <a href="https://github.com/w3c/modern-tooling/">pull requests</a>. All discussions are
            intended to be public.
          </li>
          <li>
            Go broad and assume it’s Christmas. I don’t want to discourage input by having people
            think about constraints. It’s of course very likely that at the end we can’t get
            everything but that’s okay. We can easily prioritise later, but I don’t want to miss a
            good idea.
          </li>
          <li>
            Assume hackability. Even with the best and the pithiest involved in drafting this
            document it is a given that not everything that may be useful to some part of our
            community at some moment of the future will be thought of. We need to build to ensure
            that our users can refine and extend our work.
          </li>
          <li>
            Be particularly wary of the
            <a href="http://en.wikipedia.org/wiki/Gal%C3%A1pagos_syndrome">Galápagos syndrome</a>
            (Note for PLH: this is a variant of the NIH Syndrome, but more focused on how things
            develop differently when somewhat isolated from the outside world, instead of simply
            from distaste for others’ inventions). There is every reason for W3C to have its
            specificities, even its quirks, but they need to be usable and manageable.
          </li>
          <li>
            Target mid-April for a first draft for internal W3C discussion; then present an overview
            to the AC at its early May meeting.
          </li>
          <li>
            Hopefully start implementing!
          </li>
        </ul>
      </section>
    </section>
    
    <section>
      <h2>Executive Summary &amp; Plan</h2>
      <p>
        
      </p>
      <section>
        <h2>Implementation Plan</h2>
        <p>
          XXX
        </p>
      </section>
      <!--
        XXX
        - continuous deployment for all the things
        - split the plan up by larger projects and try to estimate costs for a first wave
        - the first wave should including assessing some of the future parts further
        - make a priority hit-list for things that are most important to do this year
      -->
      <section>
        <h2>Resource Requirements</h2>
        <p>
          XXX
        </p>
      </section>
    </section>
    
    <section>
      <h2>Tools &amp; Supporting Setup</h2>
      <p>
        This section is a categorised but unprioritised list of tools and services that W3C should
        deploy, as well as ideas on how to manage integration with third-party systems.
      </p>

      <section>
        <h2>Specification Development</h2>
        <p>
          The heart of what we produce is specifications, as a manifestation of consensus. These
          already have some tooling support but it can be both further and better integrated or
          documented.
        </p>
        <section>
          <h2>Automatic Publishing</h2>
          <p>
            Great progress has already been made thanks to the
            <a href="https://github.com/w3c/echidna">Echidna</a> system that can publish Working
            Drafts in an automated manner, as often as needed. This can effectively make TR
            documents and Editor’s Drafts indistinguishable.
          </p>
          <p>
            There are, however, still a number of exceptions to the documents that Echidna can
            publish, and a number of rough edges to the system itself and its documentation. It
            needs iterative improvements.
          </p>
          <p>
            It will also need to expose a stream of notifications (as described further below) for
            the documents it publishes.
          </p>
        </section>
        <section>
          <h2>De Facto GitHub</h2>
          <p>
            While the door needs to remain open for alternatives that our users may wish to avail
            themselves of, the expected default location for specifications (and any other
            W3C-related repository) should be GitHub, preferably under the
            <a href="https://github.com/w3c/">w3c</a> organisation. This has several implications.
          </p>
          <p>
            We need to <strong>document the proper usage of GitHub for such work</strong>. Some of
            our users who are not from the Web development community can at times struggle with the
            site or with common conventions. Team resources need to be dedicated to supporting
            users of GitHub. A specific collaboration with GitHub may be considered if needed.
          </p>
          <p>
            Better even than documentation, which is often ignored, a <strong>tool that enables the
            Team to set up new repositories correctly</strong>, within conventions, and given a few
            specific options would be most useful.
          </p>
          <p>
            Groups regularly need to add new contributors to their repositories (or, on occasion, to
            remove them); which requires administrative powers. Sadly, the granularity of the ACL
            system there does not allow us to grant that power to non-Team participants, which in
            turn means that the Team needs to be called upon for everyday business. An interface to
            <strong>grant our users the ability to maintain a repository’s collaborators</strong>
            without involving the Team would be a time-saver. The ability to activate Travis without
            admin powers would also come in handy and save the Team some time.
          </p>
          <p>
            Naturally, GitHub could disappear, go bankrupt, be overtaken by an evil gang of mole
            rats, or simply fall behind. In such an event, we must have all the useful data at hand
            and, if possible, be able to exploit it elsewhere. Git certainly makes it easy to keep
            repository data — it only needs be cloned.
          </p>
          <p>
            But GitHub also has a lot of useful information that isn’t in git: issues, discussions,
            etc. We also need to keep that around. For that, we need a <strong>GitHub backup
            tool</strong>. This would use git’s naturally distributed nature in conjunction with the
            ability to receive organisation-wide hooks (and use the API to grab everything) in order
            to store all the useful information. Ideally all work would happen under the
            <a href="https://github.com/w3c/">w3c</a> organisation but the tool ought to be flexible
            enough to account for there being work elsewhere.
          </p>
          <p>
            The hook system should also be used so as to produce a stream of notifications that can
            later be integrated into the dashboard.
          </p>
          <p>
            This also entail that both <code>dev.w3</code> and <code>dvcs.w3</code> should be phased
            out. No new repository should be created there and no new user granted access. Projects
            that are still active there should be moved (and redirected). Obviously the content
            already published that no one is touching will likely need to stay there, but it should
            essentially be a static, frozen archive of old work.
          </p>
        </section>
        <section>
          <h2>Specification Styles as a Community Project</h2>
          <p>
            We regularly hear complaints about the usability of our specification styles. Several
            projects were started to radically improve them, but petered out.
          </p>
          <p>
            At the same time, they cannot be changed radically overnight. Older specifications
            cannot be broken through style changes and we shouldn’t throw away the brand recognition
            that comes with the current style.
          </p>
          <p>
            The solution is to open up the management of these styles and drive the project through
            largely small, incremental updates to the stylesheets. A repository for the styles
            should be created and we should start accepting pull requests. At first a clean-up of
            the current code ought to be carried out. Then, regular releases ought to be made
            based on contributions.
          </p>
        </section>
        <section>
          <h2>Interactivity in Specifications</h2>
          <p>
            The Web is no longer a static medium that is essentially fluid print brought to the
            screen. Documents today can work with the user interactively. So far we have only very
            sporadically made use of these capabilities, when in fact they could greatly enhance the
            usage of our primary products.
          </p>
          <p>
            A common library should be developed to provide basic functionality across all
            specifications. This would include:
          </p>
          <ul>
            <li>Simple, streamlined bug reports from within the document.</li>
            <li>Easy linking to sections.</li>
            <li>Finding for each definition where it is used.</li>
            <li>Page-width and boilerplate visibility configurability.</li>
            <li>Integration with testing &amp; coverage results.</li>
            <li>Integration with existing issues lists.</li>
          </ul>
          <p>
            Like the styling project, this can easily start small and humble, and progressively grow
            into a highly useful system thanks to contributions.
          </p>
        </section>
        <section>
          <h2>Document Structure Beyond PubRules</h2>
          <p>
            The <a href="http://www.w3.org/2014/10/pubrules/">new PubRules</a> system
            (<a href="https://github.com/w3c/specberus">Specberus</a>) is a welcome improvement in
            the toolbox. It still needs some UI fixes and should be moved to a location not intended
            to make it impossible to find but overall it should soon be able to replace the previous
            instantiation.
          </p>
          <p>
            We should, however, think beyond specification validation. As things stand today, it is
            extremely hard to get information out of our documents. Even when they are correct, they
            are all different. The RDF export is a very partial view and borderline unusable. The
            data that is extracted into W3C’s systems cannot be obtained, and is partial as well.
            Yet so much can be done with this information.
          </p>
          <p>
            We need to progressively refine the <em>components</em> that make up a specification and
            to rethink them using modern HTML constructs for document semantics and metadata. Step
            by step we need to start unifying these constructs across the board (having tools
            produce them and enforcing them through PubRules). Document structure and metadata needs
            to become eventually regular enough that TR can be used as an API.
          </p>
        </section>
        <section>
          <h2>Specification Production Tools</h2>
          <p>
            We have quality specification production tools and most specifications today make use of
            them. It is worth thinking about how, as a common ecosystem, they can be improved.
          </p>
          <p>
            One aspect in which they can be improved is to bring their source formats in line
            wherever it makes sense (and isn’t disruptive). The end goal here is that contributors
            to specifications who move between groups should as much as possible find themselves
            able to switch between tools as easily as possible.
          </p>
          <p>
            This, naturally, should not be done at the cost of innovation in those tools, but it
            surely can be done for some of the common, well-understood aspects.
          </p>
        </section>
        <section>
          <h2>Cross-References Support</h2>
          <p>
            The Web platform is increasingly described by a large mesh of specifications that
            reference one another. Currently, this is done relatively poorly overall. There are
            several methods in use:
          </p>
          <ul>
            <li>
              Handwave about doing something the way another specification says to do it, with a
              reference to the whole document.
            </li>
            <li>
              Link to a definition in another specification, which can break (as has happened
              several times for people referencing HTML), and sometimes forget to include a formal
              reference.
            </li>
            <li>
              List imported definitions from other specifications, with links and references, and
              then use internal references to those. This is likely the best option available today,
              but it is cumbersome work.
            </li>
          </ul>
          <p>
            Ideally, all definitions from all specifications should be globally available. This
            would make it possible to simply reference them in a specification tool source format
            and get the correct link and reference handled. It would enable the generation of
            glossaries (we urgently need to phase out <a href="http://www.w3.org/2003/glossary/">the
            current one</a>). We could expose a searchable interface that would make it possible for
            people to find which specification defines what, and which make use of what concept or
            construct (something that would prove invaluable in coordination).
          </p>
        </section>
        <section>
          <h2>Live Examples</h2>
          <p>
            It is extremely common today to replace examples in documentation with live ones that
            can be edited and rendered using one of the many services that provide such
            functionality.
          </p>
          <p>
            While it is probably not acceptable to inject code from a third-party service into our
            specifications, there exist reusable components that would enable examples
            in our specifications to become hackable. This would have tremendous 
            value for developers trying to learn and understand the technology.
          </p>
        </section>
        <section>
          <h2>Specref</h2>
          <p>
            <a href="http://www.specref.org/">Specref</a> is the database of
            bibliographical references that specifications rely upon. It has been through several
            iterations and is generally considered good, but it can be improved.
          </p>
          <p>
            It is currently managed by Tobie, it could be brought under W3C management.
          </p>
          <p>
            It could use a front-end for adding resources and a few other niceties to integrate
            it better with the W3C tool suite (e.g., improving Marcos' 
            [bibliography generator](http://marcoscaceres.github.io/bib_entry_maker/)).
          </p>
        </section>
        <section>
          <h2>Specification Quality Checker (CI)</h2>
          <p>
            Some specification issues (that are not about the actual prose content) can sometimes
            exist for embarrassingly long periods of time before anyone notices. Sometimes they are
            noticed after a document is finalised. Or they are raised at the worst moment, all at
            once when the editor is busy doing something else.
          </p>
          <p>
            Developers use continuous integration to make sure that there is a constant pressure
            towards quality rather than a last-minute rush — so should editors. There have been
            experiments around this (using Travis’ integration with GitHub), they should be
            expanded and systematised.
          </p>
          <p>
            Ideally, an editor should be able to, at the click of a button, have the proper Travis
            integration (and dependencies) installed into the specification’s repository and
            deployed for immediate usage. This could even be made the default at repository creation
            if it is known that it will contain a specification.
          </p>
          <p>
            Several different tools can be called up on for quality checking; since they can be used
            on their own they are listed separately below and in the «Developer Tools» section.
          </p>
        </section>
        <section>
          <h2>WebIDL Checker</h2>
          <p>
            We have an existing WebIDL Checker. It needs to be integrated into the quality workflow
            (arguably into Specberus as well as into the linting options of specification tools),
            exist with guarantees of maintenance, documentation, and made easy to find.
          </p>
        </section>
        <section>
          <h2>References Checker</h2>
          <p>
            We have existing tools (that almost no one knows about) that can read a specification
            and validate that its references (at least to W3C or IETF documents) are up to date with
            the latest versions of those documents: the
            <a href="http://www.w3.org/2004/07/references-checker-ui">references checker</a> and the
            <a href="http://www.w3.org/2007/05/ietf-references-checker">IETF references checker</a>.
          </p>
          <p>
            While specification production tools, by building atop SpecRef, have largely removed the
            need for these, some specifications are produced by hand and some have local
            bibliographies. They should be part of automated checking.
          </p>
        </section>
        <section>
          <h2>Spell Checker</h2>
          <p>
            W3C actually exposes a <a href="http://www.w3.org/2002/01/spellchecker">Spell checker</a>
            tool (that can work with HTML). It is unclear that this could be integrated into
            automated checking (given the number of justifiable exceptions) but it could perhaps be
            made more prominent (or, if found not to be useful, retired).
          </p>
        </section>
        <section>
          <h2>Specification Diffs</h2>
          <p>
            W3C currently offers an <a href="http://www.w3.org/2007/10/htmldiff">HTML Diff</a>
            service which is sometimes used to see changes to documents.
          </p>
          <p>
            It is a common request to find out what has changed between two versions of a
            specification, in fact producing a diff seems slated to be part of the new errata
            management approach that the AB is investigating.
          </p>
          <p>
            This should be made automatic, and possibly specialised. Right now the HTML diff tool
            will produce as output a complete document, which can make the differences hard to find
            (e.g. a paragraph change in the whole HTML specification). It can also include a lot of
            spurious differences (for instance if a section was added at the beginning, all the
            section numbers come out as changes).
          </p>
          <p>
            This tool could be specialised for specifications in such a way that ignorable
            differences would be ignored. Also, an affordance could be exposed in specifications
            offering to see a diff between the version being looked at and any other version of this
            document.
          </p>
        </section>
        <section>
          <h2>WYSIWYG Specifications</h2>
          <p>
            Supporting WYSIWYG editing for specifications could help improve collaboration and
            contributions from people who might know HTML but are not necessarily versed in the
            specific formalism of a given specification tool.
          </p>
          <p>
            While such a tool would be interesting and valuable, it is a long and complex project
            to put together, and likely not a priority.
          </p>
        </section>
      </section>



      <section>
        <h2>Dashboard</h2>
        <p>
          Perhaps the most common complaint about the W3C’s tooling setup, which is only made worse
          by spreading work across a broad set of external services, is that there are too many
          places at which to track things, often in very different ways.
        </p>
        <p>
          In addition to tracking information, taking action within the system also involves
          remembering where different parts are. There are several pages listing services (e.g.
          the Member page, the AC page, the Editors’ page) but they can prove overwhelming,
          especially since they often have content that is out of date. They are also very generic:
          most users don’t need to know everything that they contain.
        </p>
        <p>
          This leads to three high-level requirements for a modern system, from its users’s point of
          view:
        </p>
        <ul>
          <li>
            <strong>Unity</strong>: there needs to be a single URL at which to find
            <em>everything</em>. This should include information about what is going on in parts of
            the organisation that one is interested in, but also the ability to use the tools that
            one needs to carry out one’s work within W3C.
          </li>
          <li>
            <strong>Customisability</strong>: since no two participants have the same needs, they
            need to be able to fully customise their interface.
          </li>
          <li>
            <strong>Consistency</strong>: learning one tool is difficult, learning many all the more
            so if they all look different. As far as possible we should endeavour to expose a
            consistent user experience to our systems so as to reduce the learning curve for
            newcomers.
          </li>
        </ul>
        <p>
          This translates to a specific design for the various moving parts of the system.
        </p>
        <section>
          <h2>Unified Feeds</h2>
          <p>
            There are many sources of data that can become involved in the dashboard. In order to
            keep them manageable, we need a specialised component that can provide a consistent
            interface to them.
          </p>
          <p>
            This service is a relatively simple shell around various data streams. It can be
            configured to poll a resource at regular intervals (e.g. retrieve and process an RSS
            feed), or on the contrary to expose a hook for pushed information (e.g. we can set up
            a GitHub hook calling it for all events in the <code>w3c</code> organisation there).
            Ideally we could also collect data from mailing lists, for instance to produce a feed of
            browser vendors’ intents to implement/deprecate.
          </p>
          <p>
            Each of its plugins knows how to obtain data and store it in a unified manner. They can
            easily be configured (e.g. to add RSS feeds, to receive GitHub notifications from other
            repositories).
          </p>
          <p>
            This mass of data can then be returned to queries that essentially involve two filters:
            which feeds the user is interested in, and a date-time cut-off so that one can ask only
            for what has happened since one last visited.
          </p>
          <p>
            Additionally, a WebSockets interface can be made available so that dashboard users can
            get notifications in real time in the UI.
          </p>
        </section>
        <section>
          <h2>Dashboard</h2>
          <p>
            The dashboard itself is nothing but a container of smaller widgets which can be
            configured, added, or removed by each user. In order to be fully generic, we need to
            ensure that it remains extremely simple both in its own functionality and in the
            interface it exposes to widgets.
          </p>
          <p>
            Widgets can be used as part of an overview grid, or can take over the whole screen when
            they need more real estate.
          </p>
          <p>
            It is very important that it be possible for independent contributors to develop widgets
            separately, that can then become available through the dashboard (after W3C approval, we
            can’t automatically deploy third-party code). Widgets can expect to run in insulated
            <code>iframe</code>s and to communicate with the dashboard to access whatever
            information they need through messaging.
          </p>
          <p>
            W3C’s own applications are expected to transition to being exposed as widgets. The
            general principle at work is that services should be made available as APIs, and
            exposed to users through widgets. With this in mind widgets can use a common stylesheet
            and common scripting libraries to help support greater coherence and fluidity in
            interaction.
          </p>
        </section>
        <section>
          <h2>Notifier</h2>
          <p>
            Another tool that can plug into the unified feed is a notifier. Users can opt into
            being notified of various events by email (though, you guessed it, a dashboard widget).
          </p>
          <p>
            The notifier simply receives the unified data feed, and for each event finds if there
            are people who want to be notified of it. If so, they get an email.
          </p>
          <p>
            The notifier needs to be able to filter events based on labels. For instance, the I18N
            WG uses «I18N» labels on bugs on other groups’ bugs trackers in order to track 
            horizontal reviews. Similar conventions could be supported by other groups.
          </p>
          <p>
            Dom has built a tool
            (<a href="https://github.com/dontcallmedom/github-notify-ml">github-notify-ml</a>) that
            can notify mailing lists when certain events happen on GitHub. It could be enhanced to
            make use of this system.
          </p>
        </section>
      </section>

      <section>
        <h2>Groups &amp; Community</h2>
        <p>
          This section covers tools that are in common use for coordination and communication inside
          groups and the broader community.
        </p>
        <section>
          <h2>IRC &amp; Chatting</h2>
          <p>
            IRC is relatively operational and it is highly programmable. Because of this, it would
            be hard to replace with other more modern solutions such as
            <a href="https://slack.com/">Slack</a> or <a href="https://gitter.im/">gitter</a>. It
            is, however, often unknown to the younger generation or in fact more generally to people
            not strongly steeped in the open source culture of the past twenty years. As such, an
            improved Web interface would be desirable (the current one is serviceable, but could do
            with some freshening). One potential option here would be
            <a href="https://github.com/erming/shout">Shout</a>. It is a client-server combo with a
            rich interface, and is entirely implemented in JS. Amongst its advantages are that it
            can remain connected to IRC even when you close your browser, it supports connecting as
            the same user through multiple channels at once, and has a responsive layout that is
            well-suited to usage on a smartphone.
          </p>
          <p>
            IRC logging could, however, be improved. <code>RRSAgent</code> has its value for the
            specific case of capturing minutes in a manner that is easily reused by other tools, but
            as a general-purpose logger we can do better. Too many channels are simply not logged at
            all — even though much work takes place over IRC! And when they are logged,
            <code>RRSAgent</code> logs are then extremely hard to find; lost in dated-space never to
            be retrieved again.
          </p>
          <p>
            A general-purpose channel logger bot need not be complicated. In my experience, people
            are satisfied with <a href="http://krijnhoetmer.nl/irc-logs/">Krijn Hoetmer’s setup</a>.
            Its limitations are largely that it requires getting agreement from Krijn in order to
            get something logged, and it wouldn’t scale to the many channels we would want to log.
            The RICG also uses a simple [Drupal-based PHP setup](https://www.drupal.org/project/bot) 
            to reference discussion, retrive to GitHub issue information, and for minute-taking. 
          </p>
          <p>
            A single service at which all chat logs can be located would therefore be needed. It
            does not require bells and whistles, but it needs to support continuous logging, finding
            channels, and search.
          </p>
        </section>
        <section>
          <h2>Mailing List Interfaces</h2>
          <p>
            While the mailing list service itself is largely beyond reproach, the interface to our
            archives is now one of the more decried parts of our offering. While a number of the
            complaints may be exaggerated, it is certainly true that it could do with improvements
            in both style and usability.
          </p>
          <p>
            A project that is commonly cited as a huge improvement over what existed previously is
            <a href="https://esdiscuss.org/">https://esdiscuss.org/</a>. It is not perfect, and the
            source can probably not be reused as is. Its fundamental architecture is sound however:
            convert email to JSON and expose those for other services to build upon. Given the size
            of the W3C archive and its daily volume, this would not be a minor undertaking — it is
            likely better to test-drive the idea on a small set of lists.
          </p>
          <p>
            People have also complained about the user experience of signing up to our lists. A
            user interface allowing people who have an account (either with us, or with a
            third-party service that we can authenticate against) to use that to subscribe directly
            (without the email ping-pong) and to immediately accept archival would help our
            community as well.
          </p>
        </section>
        <section>
          <h2>Forum</h2>
          <p>
            Forums have overtaken mailing lists in terms of popularity for online discussion. Groups
            would certainly benefit from being able to rely on those for at least some of their
            work.
          </p>
          <p>
            This aspect is already covered by our
            <a href="http://discourse.specifiction.org/">Discourse</a> installation, which is being
            moved to WebPlatform.
          </p>
        </section>
        <section>
          <h2>EtherPads</h2>
          <p>
            Etherpads are commonly used to take shared notes, which <em>could</em> be useful for
            minutes in some groups, but is also particularly useful during brainstorming sessions
            when a group of people wishes to collectively.
          </p>
          <p>
            Because W3C does not provide its own pads, people routinely use those available from
            <a href="https://etherpad.mozilla.org/">Mozilla</a> or the
            <a href="http://etherpad.mit.edu/">MIT</a>. This results in information being scattered
            far and wide.
          </p>
          <p>
            There are numerous pad implementations, deploying this is largely a case of picking one
            that we like. A good pad should support:
          </p>
          <ul>
            <li>History: you can replay the edits;</li>
            <li>Archivability: you can mark a pad as «done» to freeze it.</li>
            <li>
              Search: the pads should be indexed so that one can find content. This could be
              supported externally, but does require the pad system to offer the means of listing
              all the existing pads.
            </li>
          </ul>
          <p>
            Bonus points if there is a way for the pad to tie into our user system such that people
            who join a pad to edit automatically have their names set up.
          </p>
        </section>
        <section>
          <h2>Collaborative Todos</h2>
          <p>
            This section is more of an open question than anything else. Some teams use systems like
            <a href="https://trello.com/">Trello</a> to organise their work, basically through
            collaborative todo lists. I am not aware of groups doing the same, but I would be
            interested in hearing about usage or interest.
          </p>
        </section>
        <section>
          <h2>Issue Trackers</h2>
          <p>
            Three major bug trackers are in use in W3C groups:
            <a href="https://www.w3.org/Bugs/Public/">Bugzilla</a>,
            <a href="http://www.w3.org/2005/06/tracker/">Tracker</a>, and GitHub Issues.
          </p>
          <p>
            Bugzilla is the most powerful and flexible of the three, but it is extremely slow, its
            interface is clunky, and automating it through external tools generally requires
            scraping its HTML output. (There does exist a way of enable XML-RPC for it... but, well,
            it’s XML-RPC.) It also likes to send email; a lot.
          </p>
          <p>
            Tracker is the only option with IRC integration (through <code>trackbot</code>) and it
            also supports action items in addition to issues. It knows about our users, IRC nicks,
            and working groups, which can prove useful. However its interface is also clunky and
            setting up a new instance for a given project requires intervention.
          </p>
          <p>
            GitHub issues are by far the easiest to use. They have nice conversational capabilities.
            However they’re external to W3C and they don’t know about groups and the such.
          </p>
          <p>
            I am making the following perhaps radical recommendations:
          </p>
          <ul>
            <li>
              Phase out Bugzilla. It is horribly slow and painful to use. Its code base is hopeless.
              I know of no group that uses its more advanced features; and groups that use some of
              them tend to use them wrong.
            </li>
            <li>
              Phase out Tracker. Issues should be linked to a project more than to a group. Action
              items can be issues assigned to someone, and eventually tied to a milestone (the dates
              on AIs are almost never heeded anyway).
            </li>
            <li>
              Move all issue tracking to GitHub. Having issues tied to projects makes up for tying
              them to a group. There is no requirement for administrative intervention. The hardest
              point is IRC integration. An IRC bot will need to be developed that can integrate
              with GitHub issues, can be configured directly in a channel, and can proxy action on
              behalf of people on the channel. This is not a minor project, but the upside is that
              if done well it would be of interest to a community beyond W3C and would therefore
              likely receive contributions.
            </li>
          </ul>
          <p>
            If we cannot unify our issues handling, then at the very least the various services will
            need to be plugged into the <a href="#unified-feeds">Unified Feeds</a> system so as to
            be visible in the dashboard.
          </p>
        </section>
        <section>
          <h2>The Fate of Wikis</h2>
          <p>
            Wikis are annoying. Idiosyncratic syntax, little to no notification, practically nothing
            in the way of actual collaboration (people might work on the same document but don’t
            really talk, certainly not in practice). W3C makes the problem worse by having
            uncountable different wikis spread out all over the place. It is already hard to find
            information in one wiki; when one has to remember which wiki it was in things go awry.
          </p>
          <p>
            That said, wikis do have their die-hard lovers and they will likely remain in use. I
            recommend:
          </p>
          <ul>
            <li>Avoiding the creation of new wikis as much as possible.</li>
            <li>Making the existing ones read-only as often as possible.</li>
            <li>
              If people need to produce joint notes, they can use a pad. If they need to produce
              joint documents they can use HTML in a repository, with all the usual collaboration
              this enables (issues, pull requests, etc.).
            </li>
            <li>
              For wikis that remain in use, they should be integrated into the
              <a href="#unified-feeds">Unified Feeds</a> system so that at least changes made there
              can show up in the dashboard. A single entry point to search across all the (public
              and active) wikis would be a big plus.
            </li>
          </ul>
        </section>
        <section>
          <h2>Documentation &amp; Templates</h2>
          <p>
            Even with a freshened up and more organised setup, making use of all our tools and the
            external tools they rely on can be challenging.
          </p>
          <p>
            It is absolutely necessary that there exist clear and well-identified guidelines for
            operating with these tools. Ideally, short enough that people actually read and
            memorise them.
          </p>
          <p>
            Where possible, we should provide template configurations (e.g. the typical GitHub
            repository for a specification) that can easily be reused.
          </p>
        </section>
      </section>

      <section>
        <h2>Developer Services: W3C developer tools</h2>
        <p>
          Developers use W3C technology a lot, and some of our services. Yet we make little use of
          this interaction in order to build a relationship that we badly need. There is currently
          a lost opportunity to turn these useful (and, in some cases, used) tools into goodwill
          and good feedback. We are probably not getting as many donations as we could from these.
        </p>
        <section>
          <h2>Developer Landing Page</h2>
          <p>
            We don’t offer an integrated view on the tools that we make available to developers.
            There is ongoing work in exposing this through a simple landing page.
            The <a href="http://w3c.github.io/developer-tools/">W3C developer tools page</a> aims to
            provide developers with a summary of all our open source tools and links to helpful
            resources. It is currently in development, you can
            <a href="https://github.com/w3c/developer-tools">contribute to it in its Github
            repository.</a>
          </p>
        </section>
        <section>
          <h2>Link Checker</h2>
          <p>
            The Link Checker is one of the more useful resources that W3C exposes, but it is
            woefully antiquated.
          </p>
          <p>
            While not a top priority, it would be useful to rebuild it with modern technology and a
            modern interface. It should be relatively straightforward to make it a fair bit faster,
            if only by adding some caching.
          </p>
        </section>
        <section>
          <h2>CSS &amp; HTML Validators</h2>
          <p>
            These are solid tools and do not need much work. We should stop calling the new HTML
            validator «experimental» and we should retire anything to do with the old one. We can
            sprinkle a little bit of nice design on the output, and we could document some of the
            APIs better, but nothing radical beyond some cleanup. It would be nice to ensure that
            the validators are available as libraries (outside Java), for instance as Gulp/Grunt
            packages that can easily integrate with common build systems for Web applications.
          </p>
          <p>
            It has been suggested that it would be valuable to rewrite these two validators in a
            language more likely to be reusable by Web developers, namely JavaScript. While this may
            be true, it can only be a long-term, low-priority objective.
          </p>
        </section>
        <section>
          <h2>I18N Checker</h2>
          <p>
            As in the previous section, this is good and operational. It could use being promoted
            more.
          </p>
        </section>
        <section>
          <h2>Accessibility Checker</h2>
          <p>
            We currently do not provide A11Y checking as a service, and arguments have been made
            that this would be a bad idea as there are aspects that cannot be tested. However,
            there are projects appearing that are beginning to offer this service (e.g.
            <a href="http://tenon.io/">Tenon</a>). We should take the time to think about our
            strategy here.
          </p>
        </section>
        <section>
          <h2>MobileOK</h2>
          <p>
            The <a href="http://validator.w3.org/mobile/">MobileOK</a> checker is a potentially
            valuable tool, but it needs an update. Validating XHTML Basic 1.1 or ensuring that a
            page is under 20K might not work that well with many modern uses of mobile. (Testing a
            site that I know works well on today’s devices scored it 29%.)
          </p>
          <p>
            W3C is currently working on a new, open source version of the
            <a href="https://github.com/w3c/Mobile-Checker">Mobile Checker</a>. It is expected to
            be released soon.
          </p>
        </section>
      </section>

      <section>
        <h2>Core Site</h2>
        <p>
          While not strictly a tool in itself, the core <code>w3c.org</code> site is a big part of
          what our users interact with, and improving it would help with many aspects of their work,
          notably not driving them to third parties when there is no need to.
        </p>
        <section>
          <h2>Moving away from CVS</h2>
          <p>
            CVS makes using the core site inordinately painful. I for one avoid publishing anything
            there unless I really have to; in several cases I have set up an <code>.htaccess</code>
            proxy from a subdirectory just so I would never need to touch it again. I know others
            have similar experiences.
          </p>
          <p>
            The massive size of the site also causes issues of its own. For instance, the recent
            discussion on upgrading to HTTPS across the board revealed just how hard it is to
            evolve it.
          </p>
          <p>
            The site cannot be moved to a more modern system wholesale, but likely it does not need
            to.
          </p>
          <ul>
            <li>Old content that is no longer touched can be left dormant where it is.</li>
            <li>
              The focus of the core site should be as much as possible restricted to its
              public-facing components; basically explaining what W3C is, providing some news, etc.
            </li>
            <li>
              Active content (group pages, services, etc.) should all be moved to their own
              sub-sites (e.g. using subdomains) that have their own Web space and can clearly be
              identified as their own thing. Not only does this make them easier to find, it also
              makes them manageable as smaller, modular units. These could all get their own
              individual repositories for instance.
            </li>
            <li>
              Things that have historically been maintained somewhere in the bowels of this vast
              beast should be moved one by one to another location of their own.
            </li>
          </ul>
        </section>
        <section>
          <h2>Idiosyncrasies</h2>
          <p>
            W3C invented conventions before conventions existed. In some cases, those became
            conventions only within W3C.
          </p>
          <p>
            <code>Overview.html</code> is one of those. The norm is <code>index.html</code>. It
            should become the norm on all new Web setups at W3C, and where possible
            <code>Overview.html</code> ought to be retired. It may seem like a small thing but it is
            a regular annoyance.
          </p>
          <p>
            Dated space, of course, needs to die. There are exact seven people in the world who find
            it usable, six of which are on the Team (the other one being DanC). It is maddening,
            distracting, confusing, and in violation of every single URL design best practice in
            use anywhere outside the Consortium’s walls.
          </p>
          <p>
            Again, dormant content in dated space can stay there. But content that is in any way,
            manner, or form still in active use should never be in dated space. A non-exhaustive
            list of content that should never be allowed to reside in dated space includes:
          </p>
          <ul>
            <li>Public-facing content;</li>
            <li>Groups;</li>
            <li>Charters;</li>
            <li>Tools</li>
          </ul>
        </section>
      </section>

      <section>
        <h2>Testing</h2>
        <p>
          Our testing infrastructure has become increasingly important. In general it is much more
          recent that the rest of our tools and so less in need of modernising. But the setup can
          nevertheless be improved.
        </p>
        <section>
          <h2>Critic</h2>
          <p>
            The Web Platform Tests (WPT) system relies on a code-review engine known as Critic. It
            ties into GitHub pull requests and is much more powerful than GitHub’s built-in code
            review.
          </p>
          <p>
            Most groups likely do not need Critic, but for more complex code such as that which is
            found in tests it is a very welcome improvement.
          </p>
          <p>
            The <a href="http://critic.hoppipolla.co.uk/">current Critic instance</a> is run by
            James Graham on his personal server, and without HTTPS. While that is not the end of the
            world, given how central Critic is to the testing work it would be much better to host
            it on W3C.
          </p>
        </section>
        <section>
          <h2>Automatic Browser Testing</h2>
          <p>
            We have an existing system that allows people to run the (automated parts of the) test
            suite continuously, without human intervention. Right now it has only been adopted by
            a couple of browser vendors.
          </p>
          <p>
            We should take this to the next level. We can integrate with
            <a href="https://saucelabs.com/">SauceLabs</a> (with whom we have a free account) and
            systematically run the entire test suite against the full set of browsers that they
            expose. This should allow us to gather very detailed statistics about which parts of the
            platform work where. It has the added advantage that it can help detect some broken
            tests.
          </p>
        </section>
        <section>
          <h2>Coverage &amp; Test/Spec Linking</h2>
          <p>
            Finding out how much of a given specification is covered by the test suite is a hard,
            manual, and error-prone process. It cannot be perfectly automated, but it can be
            improved.
          </p>
          <p>
            This also involves figuring out how to identify that a given test is related to part of
            a specification.
          </p>
          <p>
            These tasks are not trivial and highly heuristic, but there has been previous work. It
            needs to be assessed and integrated.
          </p>
        </section>
        <section>
          <h2>Publishing Results</h2>
          <p>
            Once we have testing results for many implementations, we should publish them in a
            manner that is useful to developers. Such information is invaluable. The interface does
            not require much in terms of complexity.
          </p>
        </section>
        <section>
          <h2>Collaborators Access</h2>
          <p>
            The testing project involves close collaborations between parties that are not all on
            the Team, yet develop the major parts of the system. While the service should be run on
            W3C resources, it would not be efficient to gate deployment to Team members only.
          </p>
          <p>
            It must be possible to enable SSH access to a select group of
            contributors so that they could help curate the testing services.
          </p>
        </section>
      </section>

      <section>
        <h2>Core Services</h2>
        <p>
          These are services that can be reused by other parts of the ecosystem.
        </p>

        <section>
          <h2>Single Sign-On</h2>
          <p>
            Many tools and services will expose features that are user-dependent. It is not
            appropriate for those to implement their own user database duplicating W3C’s. It would
            also be a shame if they had to make use of third-party sign-on infrastructure (which
            was the case with Discourse).
          </p>
          <p>
            Which exact solution is selected here is of relatively little importance, but it does
            need to support typical modern Web application authentication workflows. GitHub’s
            <a href="https://developer.github.com/v3/oauth/">OAuth implementation</a> may provide a
            good example.
          </p>
          <p>
            It should also be possible to retrieve useful information about a user, such as their
            GitHub ID.
          </p>
        </section>
        <!--
          XXX
          - ping Jose and Yves
        -->

        <section>
          <h2>Search</h2>
          <p>
            The various tools to be developed may commonly require search functionality. Given the
            spread of W3C’s content, this can quickly become required. For instance, an Etherpad
            service might request indexing for its pads (using its own ACL knowledge), a chat
            logging service might index the logs, etc.
          </p>
          <p>
            One potential candidate, that can double as a JSON document store and therefore be
            usable for application data storage, is <a href="http://www.elasticsearch.org/">Elastic
            Search</a>.
          </p>
        </section>
      </section>
    </section>

    <section>
      <h2>Deployment</h2>
      <p>
        All new tools must be developed in a such a manner that they are automatically deployed
        to production whenever a change is merged to master. It is the only sane way to maintain
        a product.
      </p>
      <p>
        There is little point in producing new tools if they are hard to use and people do not 
        understand them. It is an absolute requirement for tools being deployed to be accompanied by
        genuine, non-cursory documentation intended at all their possible users.
      </p>
    </section>

    <section>
      <h2>Principles for External Tool Integration</h2>
      <p>
        It is this plans’s considered opinion that users should be encouraged to experiment with
        external tools if and when they find shortcomings in ours. Having said that, for an external
        tool to be considered acceptable it needs to fulfil the following basic needs:
      </p>
      <ul>
        <li>Accessibility;</li>
        <li>
          Internationalisation (not necessarily localised to all languages, but at least usable with 
          content from those languages which Unicode supports);
          </li>
        <li>Availability in all countries;</li>
        <li>Reasonable expectations of privacy and security.</li>
      </ul>
      <p>
        Additionally, the W3C has requirements in terms of persistence. We cannot require of an
        external service that it make a promise of persistence it may be unable to keep, but we need
        to have the means to obtain our data easily so as to persist it easily ourselves. This has
        the nice side property that it requires full data export and ownership.
      </p>
    </section>
    
    <section>
      <h2>Acknowledgements</h2>
      <p>
        The following people have provided useful input into this document.
      </p>
      <ul>
        <li>Dave Raggett</li>
        <li>Dave Singer</li>
        <li>Ivan Herman</li>
        <li>James Fuller</li>
        <li>Jeff Jaffe</li>
        <li>Philippe Le Hégaret</li>
        <li>Ralph Swick</li>
        <li>Rob Sanderson</li>
        <li>Tobie Langel</li>
        <li>Marcos Caceres</li>
      </ul>
    </section>
    <!--
      Schedule
      - March 11, W3M
      - March 12, Global
      - April 15, W3M report
      - April 20, AB
      - May 6 AC
    -->
  </body>
</html>
